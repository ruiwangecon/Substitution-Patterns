{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7335f284",
   "metadata": {},
   "source": [
    "## Simulation code for the project 'Testing and Identifying Substitution and Complementarity Patterns' \n",
    "\n",
    "It is implemneting four different estimators for the structural demand estimation model\n",
    "- Estimator I: two-step estimator proposed in the paper\n",
    "- Estimator II: mixed-effect parametric estimator\n",
    "- Estimator III: fixed-effect logit estimator without bundles\n",
    "- Estimator IV: nonparametric estimator without bundles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2464416",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import packages\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "\n",
    "np.random.seed(1234)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887ccb1",
   "metadata": {},
   "source": [
    "### Initial configuration for simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381f876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DGP parameters\n",
    "B = 500                 # Number of simulations\n",
    "N = 1000                # Sample size\n",
    "T = 2                   # Two periods panel\n",
    "dx = 2                  # Dimension of X\n",
    "dz = 2                  # Dimension of Z\n",
    "\n",
    "# True coefficients\n",
    "beta0 = np.ones((dx, 1))    # True coefficient for a single good\n",
    "gamma0 = np.ones((dz, 1))   # True coefficient for incremental utility\n",
    "\n",
    "# Coefficients for fixed effects\n",
    "eta1 = 0.5 * np.ones((dx, 1))\n",
    "eta0 = 0\n",
    "\n",
    "# Two-step estimator in the paper\n",
    "beta_twostep = np.ones((dx - 1, B))  # Two-step estimator for beta\n",
    "gamma_twostep = np.ones((dz - 1, B)) # Two-step estimator for gamma\n",
    "err = np.ones(B)                     # Percentage of having wrong signs of substitution patterns\n",
    "\n",
    "# Parametric estimator \n",
    "beta_par = np.ones((dx - 1, B))\n",
    "gamma_par = np.ones((dz - 1, B))\n",
    "err_par = np.ones(B)\n",
    "\n",
    "# Two estimators assuming no bundles\n",
    "beta_non = np.ones((dx - 1, B))  # Estimator under stationarity but assuming no bundles\n",
    "beta_fl = np.ones((dx - 1, B))   # Fixed effect logit estimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe407ae",
   "metadata": {},
   "source": [
    "### Helper functions to help define moment functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9451e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sign function\n",
    "def sg(y):\n",
    "    return np.sign(y)\n",
    "\n",
    "## differencing function for y\n",
    "def D(x):\n",
    "    return x[:, dx:] - x[:, :dx]\n",
    "\n",
    "## differencing function for y\n",
    "def dp(y):\n",
    "    return y[:, 1] - y[:, 0]\n",
    "\n",
    "## absolute function\n",
    "def G(x):\n",
    "    return np.abs(x)\n",
    "\n",
    "## positive value function\n",
    "def G1(x):\n",
    "    return x * (x >= 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae24c58",
   "metadata": {},
   "source": [
    "### First-step estimator to predict consumers' choice \n",
    "- implement a single-layer neural network estimator and a random forest estimator\n",
    "- use cross-validation to choose tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03f01c",
   "metadata": {},
   "source": [
    "#### First-step estimator I: neural network estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_loss(y_true, y_pred):\n",
    "    \"\"\"Compute log loss for each binary target and return the average log loss.\"\"\"\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) for i in range(y_true.shape[1])])\n",
    "\n",
    "epoch_candidates = [50, 100, 200]               # number of iterations\n",
    "learning_rate_candidates = [0.001, 0.01]   # learning rate #, 0.1\n",
    "\n",
    "def neu(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: input features which is an numpy array of shape (N, k)\n",
    "    - y: N *dependent variable which is an numpy array of shape (N, s)\n",
    "   \n",
    "    Output:\n",
    "    - predicted_probabilities: predicted probabilities for each y\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits = 3, shuffle=True, random_state=42)   # kfold CV\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_epochs = 0\n",
    "    best_learning_rate = 0.0\n",
    "    \n",
    "    for epochs in epoch_candidates:\n",
    "        for lr in learning_rate_candidates:\n",
    "            fold_losses = []\n",
    "            \n",
    "            for train_index, val_index in kf.split(X):\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "                \n",
    "                # build the model\n",
    "                model = Sequential([Input(shape=(X.shape[1],)),\n",
    "                                    Dense(units=y.shape[1], activation='sigmoid')])\n",
    "                model.compile(optimizer=Adam(learning_rate=lr), \n",
    "                              loss='binary_crossentropy', \n",
    "                              metrics=['accuracy'])\n",
    "                # fit the model\n",
    "                model.fit(X_train, y_train, epochs=epochs, batch_size=X_train.shape[0], verbose=0)\n",
    "                \n",
    "                # compute prediction and loss\n",
    "                y_val_pred = model.predict(X_val)\n",
    "                val_loss = compute_log_loss(y_val, y_val_pred)\n",
    "                fold_losses.append(val_loss)\n",
    "            \n",
    "            # compute average loss and update the best model\n",
    "            avg_loss = np.mean(fold_losses)\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_epochs = epochs\n",
    "                best_learning_rate = lr\n",
    "                best_model = model\n",
    "    \n",
    "    # Train the best model on the entire dataset\n",
    "    best_model.fit(X, y, epochs=best_epochs, batch_size=X.shape[0], verbose=0)\n",
    "    \n",
    "    # Predict probabilities\n",
    "    predicted_probabilities = best_model.predict(X)\n",
    "    \n",
    "    return predicted_probabilities \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83503a60",
   "metadata": {},
   "source": [
    "#### First-step estimator II: random forest estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2097e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: input features which is an numpy array of shape (N, k)\n",
    "    - y: N *dependent variable which is an numpy array of shape (N, s)\n",
    "   \n",
    "    Output:\n",
    "    - predicted_probabilities: predicted probabilities for each y\n",
    "    \"\"\"\n",
    "    n_estimators_candidates = [200, 300, 500]  # number of n_estimators\n",
    "    max_depth_candidates = [10, 15, 20]        # number of depth\n",
    "    \n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    \n",
    "    for n_estimators in n_estimators_candidates:\n",
    "        for max_depth in max_depth_candidates:\n",
    "            fold_losses = []\n",
    "            \n",
    "            for train_index, val_index in kf.split(X):\n",
    "                X_train, X_val = X[train_index], X[val_index]\n",
    "                y_train, y_val = y[train_index], y[val_index]\n",
    "                \n",
    "                # build a model\n",
    "                model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                # compute prediction and loss\n",
    "                y_val_pred = model.predict_proba(X_val)  \n",
    "                y_val_pred_proba = np.array([y_val_pred[:, 1] for y_val_pred in y_val_pred]).T\n",
    "                \n",
    "                val_loss = compute_log_loss(y_val, y_val_pred_proba)\n",
    "                fold_losses.append(val_loss)\n",
    "            \n",
    "            # compute average loss and update the best model\n",
    "            avg_loss = np.mean(fold_losses)\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                best_model = model\n",
    "                best_params = {'n_estimators': n_estimators, 'max_depth': max_depth}\n",
    "    \n",
    "    # Train the best model on the entire dataset\n",
    "    best_model.fit(X, y)\n",
    "    \n",
    "    # Predict probabilities on the entire dataset\n",
    "    predicted_probabilities = np.array([probs[:, 1] for probs in best_model.predict_proba(X)]).T\n",
    "    \n",
    "    return predicted_probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ab5d0b",
   "metadata": {},
   "source": [
    "### Estimator I: two-step estimator in this paper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b492619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define moment equality function\n",
    "\n",
    "def mom(theta, XA, XB, Z, p):\n",
    "    '''\n",
    "    theta: model parameter\n",
    "    XA(XB): covariate for good A(B) at all periods\n",
    "    p: estimated choice probability for Y1, Y2, Y3, Y0\n",
    "    '''\n",
    "    beta = np.concatenate(([1], theta[:dx - 1]))\n",
    "    gamma = np.concatenate(([1], theta[dx - 1:]))\n",
    "    Gamma = Z @ gamma  # Incremental utility\n",
    "\n",
    "    deltaA = D(XA) @ beta  # Variation in covariate index for good A\n",
    "    deltaB = D(XB) @ beta  # Variation in covariate index for good B\n",
    "\n",
    "    # Extracting probabilities\n",
    "    p1 = p[:, :T]               # Estimated choice probability for choosing only good A\n",
    "    p2 = p[:, T:2 * T]          # Estimated choice probability for choosing only good B\n",
    "    p0 = p[:, 2 * T:3 * T]      # Estimated choice probability for outside option\n",
    "    p3 = p[:, 3 * T:4 * T]      # Estimated choice probability for buying AB\n",
    "    p13 = p1 + p3               # Demand for good A\n",
    "    p23 = p2 + p3               # Demand for good B\n",
    "\n",
    "    # Construct index for a single good\n",
    "    I1 = 1 - ((sg(dp(p1)) * deltaA > 0) | (sg(dp(p1)) * deltaB < 0))\n",
    "    I2 = 1 - ((sg(dp(p2)) * deltaA < 0) | (sg(dp(p2)) * deltaB > 0))\n",
    "    I3 = 1 - ((sg(dp(p3)) * deltaA > 0) | (sg(dp(p3)) * deltaB > 0))\n",
    "    I0 = 1 - ((sg(dp(p0)) * deltaA < 0) | (sg(dp(p0)) * deltaB < 0))\n",
    "\n",
    "    # Objective function for single choice\n",
    "    Q1 = np.mean(G1(dp(p1)) * I1 + G1(dp(p2)) * I2 + G1(dp(p3)) * I3 + G1(dp(p0)) * I0)\n",
    "\n",
    "    # Construct index for demand of good A and good B\n",
    "    I13 = 1 - ((sg(dp(p13)) * deltaA > 0) | (sg(dp(p13)) * (deltaA + sg(Gamma) * deltaB) > 0))\n",
    "    I23 = 1 - ((sg(dp(p23)) * deltaB > 0) | (sg(dp(p23)) * (deltaB + sg(Gamma) * deltaA) > 0))\n",
    "\n",
    "    # Objective function using conditional demand\n",
    "    Q2 = np.mean(G1(dp(p13)) * I13 + G1(dp(p23)) * I23)\n",
    "\n",
    "    # Functions for sum of two choice probabilities\n",
    "    def Ib1(t1):\n",
    "        return 1 - ((Gamma < np.minimum((-1) ** t1 * deltaA, (-1) ** (t1 - 1) * deltaB)) &\n",
    "                ((-1) ** t1 * (deltaA - deltaB) > 0))\n",
    "\n",
    "    def Ib2(t1):\n",
    "        return 1 - ((Gamma > np.maximum((-1) ** (t1 - 1) * deltaA, (-1) ** (t1 - 1) * deltaB)) &\n",
    "                ((-1) ** t1 * (deltaA + deltaB) > 0))\n",
    "\n",
    "    def L1(t1, t2):\n",
    "        return p1[:, t1-1] + p2[:, t2-1] - 1\n",
    "\n",
    "    def L2(t1, t2):\n",
    "        return p3[:, t1-1] + p0[:, t2-1] - 1\n",
    "\n",
    "    Q3 = np.mean(G1(L1(2, 1)) * Ib1(2) + G1(L1(1, 2)) * Ib1(1) + G1(L2(2, 1)) * Ib2(2) + G1(L2(1, 2)) * Ib2(1))\n",
    "\n",
    "    # Final objective function\n",
    "    return Q1 + Q2 + Q3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5415917",
   "metadata": {},
   "source": [
    "### Estimator II: Mixed-effect logit estimator\n",
    "- assume a parametric model \n",
    "- use the simulated method of moments to calculate choice probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bad774-6600-4b83-a513-e15a317c472f",
   "metadata": {},
   "source": [
    "#### 1. generate fixed effects and error terms according to mixed-effect logit model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f93d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate fixed effects and error terms according to mixed-effect logit model\n",
    "\n",
    "S = 20                                       # Number of simulations\n",
    "ep0_s = np.random.gumbel(0, 1, (N, T, S))    # error for outside option\n",
    "epA_s = np.random.gumbel(0, 1, (N, T, S))    # error for good A\n",
    "epB_s = np.random.gumbel(0, 1, (N, T, S))    # error for good B\n",
    "\n",
    "# Simulated error terms in fixed effects for goods A and B\n",
    "vA_s = np.random.randn(N, S)\n",
    "vB_s = np.random.randn(N, S)\n",
    "\n",
    "# Sum of two error terms: epA + vA and epB + vB\n",
    "muA_s = np.zeros((N, T, S))\n",
    "muB_s = np.zeros((N, T, S))\n",
    "\n",
    "# Calculating the sum of error terms for each period and simulation\n",
    "for t in range(T):  # Adjusting for Python's 0-based indexing\n",
    "    muA_s[:, t, :] = vA_s + epA_s[:, t, :] - ep0_s[:, t, :]\n",
    "    muB_s[:, t, :] = vB_s + epB_s[:, t, :] - ep0_s[:, t, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2b7bf8",
   "metadata": {},
   "source": [
    "#### 2. define criterion function for Estimator II\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5562012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Criterion function for the parametric method\n",
    "def par_sim(theta, XA, XB, Z, Y1, Y2, Y3):\n",
    "    \"\"\"\n",
    "    - theta: true parameter\n",
    "    - XA(XB): covariate for good A(B) at all periods\n",
    "    - Z: covariate for substitution patterns\n",
    "    - Y1, Y2, Y3: dependent variables\n",
    "    \"\"\"\n",
    "    # Extract parameters from theta\n",
    "    beta = np.concatenate(([1], theta[:dx-1]))\n",
    "    gamma = np.concatenate(([1], theta[dx-1:dx+dz-2]))\n",
    "    eta1 = theta[dx+dz-2:-1]\n",
    "    eta0 = theta[-1]\n",
    "\n",
    "    # Mean fixed effect for goods A and B\n",
    "    alphaA = eta0 + np.dot((XA[:, :dx] + XA[:, dx:]) / T, eta1)\n",
    "    alphaB = eta0 + np.dot((XB[:, :dx] + XB[:, dx:]) / T, eta1)\n",
    "\n",
    "    # Mean utility for goods A and B\n",
    "    delA = (np.column_stack((XA[:, :dx] @ beta, XA[:, dx:] @ beta)) + alphaA[:, None])[:, :, None]  # Mean utility for good A\\\\\n",
    "    delB = (np.column_stack((XB[:, :dx] @ beta, XB[:, dx:] @ beta)) + alphaB[:, None])[:, :, None]  # Mean utility for good B\n",
    "\n",
    "    # Incremental utility\n",
    "    Gam = np.dot(Z, gamma)[:, None, None]\n",
    "\n",
    "    # Simulated choice probabilities\n",
    "    p1_s = np.mean((muA_s + delA >= np.maximum(muB_s + delB, 0)) & (muB_s <= -delB - Gam), axis = 2)\n",
    "    p2_s = np.mean((muB_s + delB >= np.maximum(muA_s + delA, 0)) & (muA_s <= -delA - Gam), axis = 2)\n",
    "    p3_s = np.mean((muA_s + delA + Gam >= np.maximum(-muB_s - delB, 0)) & (muB_s >= -delB - Gam), axis=2)\n",
    "\n",
    "    # Concatenate covariates\n",
    "    X = np.hstack([XA, XB, Z])\n",
    "\n",
    "    # Moment conditions for the two periods\n",
    "    Q_s1 = np.hstack([np.dot(X.T, (Y1 - p1_s)[:, 0]), np.dot(X.T, (Y2 - p2_s)[:, 0]), np.dot(X.T, (Y3 - p3_s)[:, 0])]) / N\n",
    "    Q_s2 = np.hstack([np.dot(X.T, (Y1 - p1_s)[:, 1]), np.dot(X.T, (Y2 - p2_s)[:, 1]), np.dot(X.T, (Y3 - p3_s)[:, 1])]) / N\n",
    "    \n",
    "    # Sum of squares of the moment conditions\n",
    "    Q = np.dot(Q_s1.T, Q_s1) + np.dot(Q_s2.T, Q_s2)\n",
    "    \n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a090d4",
   "metadata": {},
   "source": [
    "### Estimator III: Chamberlain's fixed-effect logit model which does not allow bundles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1eced6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define moment function\n",
    "\n",
    "def momlogit(beta, XA, XB, Y):\n",
    "    \"\"\"\n",
    "    - beta: true parameter (excluding the constant term)\n",
    "    - XA(XB): covariate for good A(B) at all periods\n",
    "    - Y: dependent variables\n",
    "    \"\"\"\n",
    "    \n",
    "    b = np.concatenate(([1], beta))\n",
    "\n",
    "    # Indicator function for choosing c1 at t=1 and c2 at t=2\n",
    "    def dc(c1, c2):\n",
    "        return ((Y[:, 0] == c1) & (Y[:, 1] == c2)).astype(int)\n",
    "\n",
    "    # Choice probability calculations using the logistic function\n",
    "    p01 = np.exp(D(XA) @ b) / (1 + np.exp(D(XA) @ b))\n",
    "    p02 = np.exp(D(XB) @ b) / (1 + np.exp(D(XB) @ b))\n",
    "    p12 = np.exp((D(XB) - D(XA)) @ b) / (1 + np.exp((D(XB) - D(XA)) @ b))\n",
    "\n",
    "    # Calculating the log-likelihood for each pair of choices\n",
    "    Q1 = np.mean(dc(0, 1) * np.log(p01) + dc(1, 0) * np.log(1 - p01))\n",
    "    Q2 = np.mean(dc(0, 2) * np.log(p02) + dc(2, 0) * np.log(1 - p02))\n",
    "    Q3 = np.mean(dc(1, 2) * np.log(p12) + dc(2, 1) * np.log(1 - p12))\n",
    "\n",
    "    # Return the negative log-likelihood function\n",
    "    return -(Q1 + Q2 + Q3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df3ff8",
   "metadata": {},
   "source": [
    "### Estimator IV: Nonparametric estimator which does not allow bundles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd21ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define moment equality function for this estimator\n",
    "\n",
    "def mom1(beta, XA, XB, p):\n",
    "    \"\"\"\n",
    "    - beta: true parameter \n",
    "    - XA(XB): covariate for good A(B) at all periods\n",
    "    - p: estimated choice probability for four choices\n",
    "    \"\"\"\n",
    "    b = np.concatenate(([1], beta))  # Prepend 1 to beta to include intercept\n",
    "\n",
    "    # Variation in covariate index for goods A and B\n",
    "    deltaA = D(XA) @ b\n",
    "    deltaB = D(XB) @ b\n",
    "\n",
    "    # Estimated choice probabilities\n",
    "    p1 = p[:, :T]                    # For choosing only good A\n",
    "    p2 = p[:, T:2*T]                 # For choosing only good B\n",
    "    p0 = p[:, 2*T:3*T]               # For choosing neither\n",
    "\n",
    "    # Construct index for a single good\n",
    "    I1 = 1 - ((sg(dp(p1)) * deltaA > 0) | (sg(dp(p1)) * (deltaA - deltaB) > 0))\n",
    "    I2 = 1 - ((sg(dp(p2)) * deltaB > 0) | (sg(dp(p2)) * (deltaB - deltaA) > 0))\n",
    "    I0 = 1 - ((sg(dp(p0)) * deltaA < 0) | (sg(dp(p0)) * deltaB < 0))\n",
    "\n",
    "    # Construct objective function using a single choice\n",
    "    Q = np.mean(G(dp(p1)) * I1 + G(dp(p2)) * I2 + G(dp(p0)) * I0)\n",
    "    \n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788e21d",
   "metadata": {},
   "source": [
    "### Simulation starts here\n",
    "- implement four different estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdbbc4f-d2d8-4383-9ae0-b87caf350a1a",
   "metadata": {},
   "source": [
    "### Performance comparison of the four estimators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d88233",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in range(B):\n",
    "\n",
    "    # Generate covariates for good A\n",
    "    XA1 = multivariate_normal.rvs(mean=np.zeros(dx), cov=np.eye(dx), size=N)\n",
    "    XA2 = multivariate_normal.rvs(mean=np.zeros(dx), cov=np.eye(dx), size=N)\n",
    "    XA = np.column_stack((XA1, XA2))\n",
    "    \n",
    "    # Generate covariates for good B\n",
    "    XB1 = multivariate_normal.rvs(mean=np.zeros(dx), cov=np.eye(dx), size=N)\n",
    "    XB2 = multivariate_normal.rvs(mean=np.zeros(dx), cov=np.eye(dx), size=N)\n",
    "    XB = np.column_stack((XB1, XB2))\n",
    "\n",
    "    # Generate substitution pattern covariates\n",
    "    Z = np.column_stack((norm.rvs(loc=2, scale=2, size=N), norm.rvs(loc=0, scale=1, size=N)))\n",
    "\n",
    "    # all covariates\n",
    "    X = np.column_stack((XA, XB, Z))\n",
    "\n",
    "    # Error terms and fixed effects setup\n",
    "    rho = -0.7\n",
    "    cov = [[1, rho], [rho, 1]]\n",
    "\n",
    "    # Generate random samples\n",
    "    ep1 = multivariate_normal.rvs([2, -2], cov, N)\n",
    "    ep2 = multivariate_normal.rvs([2, -2], cov, N)\n",
    "    epA = np.column_stack((ep1[:, 0], ep2[:, 0]))\n",
    "    epB = np.column_stack((ep1[:, 1], ep2[:, 1]))\n",
    "    ep0 = np.random.randn(N, T)\n",
    "    alphaA = ((XA1 + XA2 - 2 * (XB1 + XB2)) @ beta0 / 4) * (1 + np.random.randn(N, 1))\n",
    "    alphaB = ((XB1 + XB2 - 2 * (XA1 + XA2)) @ beta0 / 4) * (1 + np.random.randn(N, 1))\n",
    "\n",
    "    # Latent utilities and choices\n",
    "    uA = np.hstack((XA[:, :dx] @ beta0, XA[:, dx:] @ beta0)) + alphaA + epA - ep0\n",
    "    uB = np.hstack((XB[:, :dx] @ beta0, XB[:, dx:] @ beta0)) + alphaB + epB - ep0\n",
    "    u0 = np.zeros((N, T))\n",
    "    uAB = uA + uB + Z @ gamma0\n",
    "\n",
    "    um = np.maximum.reduce([uA, uB, uAB, np.zeros((N, T))])  # maximum utility\n",
    "    \n",
    "    # generate dependent variable\n",
    "    Y = np.where(um == uA, 1, 0) + np.where(um == uB, 2, 0) + np.where(um == uAB, 3, 0)\n",
    "    Yall = np.column_stack(((Y == 1).astype(int), (Y == 2).astype(int), (Y == 0).astype(int), (Y == 3).astype(int)))\n",
    "    \n",
    "    # First step estimator - Assuming function definition for 'neu'\n",
    "    phat = neu(X, Yall)  # You need to define 'neu' to accept these inputs or adjust the call accordingly\n",
    "\n",
    "    # Two-step estimator\n",
    "    def objective_two_step(theta):\n",
    "        return mom(theta, XA, XB, Z, phat)  # Define 'mom' as per your model\n",
    "\n",
    "    initial_two_step = np.concatenate([np.ones(dx - 1), 0.5 * np.ones(dz - 1)])\n",
    "    result_two_step = minimize(objective_two_step, initial_two_step, method = 'Powell')\n",
    "    beta_twostep[:, b] = result_two_step.x[:dx - 1]\n",
    "    gamma_twostep[:, b] = result_two_step.x[dx - 1:dx + dz - 2]\n",
    "    err[b] = np.mean(np.abs((Z @ gamma0 >= 0).astype(int) - (Z @ np.concatenate([[1], gamma_twostep[:, b]]) >= 0).astype(int)))\n",
    "\n",
    "    # Mixed effect parametric estimator\n",
    "    def objective_par(theta):\n",
    "        return par_sim(theta, XA, XB, Z, Y == 1, Y == 2, Y == 3)  # Define 'par_sim' as per your model\n",
    "\n",
    "    initial_par = np.concatenate([np.ones(dx + dz), 0.5 * np.ones(dx - 1)])\n",
    "    result_par = minimize(objective_par, initial_par, method = 'Nelder-Mead')\n",
    "    beta_par[:, b] = result_par.x[:dx - 1]\n",
    "    gamma_par[:, b] = result_par.x[dx - 1:dx + dz - 2]\n",
    "    err_par[b] = np.mean(np.abs((Z @ gamma0 >= 0).astype(int) - (Z @ np.concatenate([[1], gamma_par[:, b]]) >= 0).astype(int)))\n",
    "\n",
    "    # Logit estimator without bundles\n",
    "    def objective_logit(beta):\n",
    "        return momlogit(beta, XA, XB, Y)  # Define 'momlogit' as per your model\n",
    "\n",
    "    result_logit = minimize(objective_logit, np.array([0]), method = 'Powell', bounds=[(-5, 5)])\n",
    "    beta_fl[:, b] = result_logit.x[0]\n",
    "\n",
    "    # Nonparametric estimator without bundles\n",
    "    phat1 = phat[:, :3 * T]\n",
    "    def objective_non(beta):\n",
    "        return mom1(beta, XA, XB, phat1)  # Define 'mom1' as per your model\n",
    "\n",
    "    result_non = minimize(objective_non, np.array([0]), method = 'Powell', bounds=[(-5, 5)])\n",
    "    beta_non[:, b] = result_non.x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Evaluation function of estimators\n",
    "def M(theta):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of estimators\n",
    "    - theta: array of estimator values\n",
    "    Returns: [Bias, SD, RMSE, MAD]\n",
    "    \"\"\"\n",
    "    mean_theta = np.mean(theta)\n",
    "    std_theta = np.std(theta)\n",
    "    rmse = np.sqrt(np.var(theta) + (mean_theta - 1)**2)\n",
    "    mad = np.median(np.abs(theta - 1))\n",
    "    return [round(mean_theta - 1, 3), round(std_theta, 3), round(rmse, 3), round(mad, 3)]\n",
    "\n",
    "# Performance of estimators\n",
    "rMSEnon = M(beta_twostep)\n",
    "rMSEpar = M(beta_par)\n",
    "rMSEnon1 = M(beta_non)\n",
    "rMSEfl = M(beta_fl)\n",
    "rMSEgnon = M(gamma_twostep)\n",
    "rMSEgpar = M(gamma_par)\n",
    "\n",
    "# Printing the results\n",
    "print(\"Performance of beta_twostep:\", rMSEnon)\n",
    "print(\"Performance of beta_par:\", rMSEpar)\n",
    "print(\"Performance of beta_non:\", rMSEnon1)\n",
    "print(\"Performance of beta_fl:\", rMSEfl)\n",
    "print(\"Performance of gamma_twostep:\", rMSEgnon)\n",
    "print(\"Performance of gamma_par:\", rMSEgpar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf293b03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
